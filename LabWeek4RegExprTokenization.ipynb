{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "887071\n"
     ]
    }
   ],
   "source": [
    "# get the book Emma from the Gutenberg collection and keep as raw text\n",
    "file0 = nltk.corpus.gutenberg.fileids( ) [0]\n",
    "emmatext = nltk.corpus.gutenberg.raw(file0)\n",
    "print(type(emmatext))\n",
    "print(len(emmatext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 20 characters in the str emmatext as one string\n",
    "print(emmatext[:50])\n",
    "emmatext[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "E\n",
      "m\n",
      "m\n",
      "a\n",
      " \n",
      "b\n",
      "y\n",
      " \n",
      "J\n",
      "a\n",
      "n\n",
      "e\n",
      " \n",
      "A\n",
      "u\n",
      "s\n",
      "t\n",
      "e\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "# print the first 20 characters in emmatext by iterating over the characters\n",
    "for c in emmatext[:20]:\n",
    "  print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monty PythonHoly Grail\n",
      "Monty Python and the Holy Grail\n"
     ]
    }
   ],
   "source": [
    "## Review of strings and string operation +\n",
    "string1 = 'Monty Python'\n",
    "string2 = 'Holy Grail'\n",
    "print(string1 + string2)\n",
    "print(string1 + ' and the ' + string2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Emma by Jane Austen 1816]  VOLUME I  CHAPTER I   Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace end-of-line character with a space\n",
    "# check table 3.2 in NLTK book for other string functions\n",
    "newemmatext = emmatext.replace('\\n', ' ')\n",
    "newemmatext[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Development of regular expressions for tokenizing text\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'book', 'is', 'interesting']\n"
     ]
    }
   ],
   "source": [
    "# pattern to match words, i.e. anything with a sequence of word characters, ignores special chars\n",
    "shorttext = 'That book is interesting.'\n",
    "pword = re.compile('\\w+')\n",
    "print(re.findall(pword, shorttext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U', 'S', 'A', 'poster', 'print', 'costs', '12', '40', 'but', 'with', '10', 'off']\n"
     ]
    }
   ],
   "source": [
    "specialtext = 'That U.S.A. poster-print costs $12.40, but with 10% off.'\n",
    "print(re.findall(pword, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('That', ''), ('U', ''), ('S', ''), ('A', ''), ('poster-print', '-print'), ('costs', ''), ('12', ''), ('40', ''), ('but', ''), ('with', ''), ('10', ''), ('off', '')]\n",
      "[('end-of-line', '-line'), ('character', '')]\n"
     ]
    }
   ],
   "source": [
    "# pattern to match words with internal hyphens\n",
    "ptoken = re.compile('(\\w+(-\\w+)*)')\n",
    "print(re.findall(ptoken, specialtext))\n",
    "print(re.findall(ptoken, 'end-of-line character'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U', 'S', 'A', 'poster-print', 'costs', '12', '40', 'but', 'with', '10', 'off']\n",
      "['end-of-line', 'character']\n"
     ]
    }
   ],
   "source": [
    "# ignore the group of the inner parentheses \n",
    "ptoken = re.compile('(\\w+(?:-\\w+)*)')\n",
    "print(re.findall(ptoken, specialtext))\n",
    "print(re.findall(ptoken, 'end-of-line character'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['U.S.A.']\n"
     ]
    }
   ],
   "source": [
    "# abbreviations like U.S.A.\n",
    "pabbrev = re.compile('((?:[A-Z]\\.)+)')\n",
    "print(re.findall(pabbrev, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U', 'S', 'A', 'poster-print', 'costs', '12', '40', 'but', 'with', '10', 'off']\n"
     ]
    }
   ],
   "source": [
    "# combine this pattern with the words to make more general tokens\n",
    "ptoken = re.compile('(\\w+(?:-\\w+)*|(?:[A-Z]\\.)+)')\n",
    "print(re.findall(ptoken, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U.S.A.', 'poster-print', 'costs', '12', '40', 'but', 'with', '10', 'off']\n"
     ]
    }
   ],
   "source": [
    "# switch the order of the patterns to first match abbreviations and then other words\n",
    "ptoken = re.compile('((?:[A-Z]\\.)+|\\w+(?:-\\w+)*)')\n",
    "print(re.findall(ptoken, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', 'but', 'with', '10', 'off']\n"
     ]
    }
   ],
   "source": [
    "# add expression for currency\n",
    "ptoken = re.compile('((?:[A-Z]\\.)+|\\w+(?:-\\w+)*|\\$?\\d+(?:\\.\\d+)?)')\n",
    "print(re.findall(ptoken, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 'That', ''), ('U.S.A.', '', ''), ('', 'poster-print', ''), ('', 'costs', ''), ('', '', '$12.40'), ('', 'but', ''), ('', 'with', ''), ('', '10', ''), ('', 'off', '')]\n"
     ]
    }
   ],
   "source": [
    "# this is an equivalent regular expression except that it has extra parentheses\n",
    "# the python string triple quote allows multi-line strings with end of line comments\n",
    "ptoken = re.compile(r'''((?:[A-Z]\\.)+) # abbreviations, e.g. U.S.A.\n",
    "   | (\\w+(?:-\\w+)*) # words with internal hyphens\n",
    "   | (\\$?\\d+(?:\\.\\d+)?) # currency, like $12.40\n",
    "   ''', re.X) # verbose flag\n",
    "print(re.findall(ptoken, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### using NLTK's regular expression tokenizer\n",
    "# first define a multi-line string that is a regular expression\n",
    "pattern = r''' (?x) \t# set flag to allow verbose regexps\n",
    "        (?:[A-Z]\\.)+    # abbreviations, e.g. U.S.A.\n",
    "        | \\$?\\d+(?:\\.\\d+)?%?    # currency and percentages, $12.40, 50%\n",
    "        | \\w+(?:-\\w+)*  # words with internal hyphens\n",
    "        | \\.\\.\\.        # ellipsis\n",
    "        | [][.,;”’?():-_%#’]    # separate tokens\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'book', 'is', 'interesting', '.']\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', ',', 'but', 'with', '10%', 'off', '.']\n"
     ]
    }
   ],
   "source": [
    "# the nltk regular expression tokenizer compiles the re pattern, applies it to the text\n",
    "#  and uses the matching groups to return a list of only the matched tokens\n",
    "print(nltk.regexp_tokenize(shorttext, pattern))\n",
    "print(nltk.regexp_tokenize(specialtext, pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U.S.A.', 'poster-print', 'costs', '$', '12.40', ',', 'but', 'with', '10', '%', 'off', '.']\n"
     ]
    }
   ],
   "source": [
    "# compare with built-in word tokenizer\n",
    "print(nltk.word_tokenize(specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenizer for Twitter derived tweetmotif from the ARK, developed at CMU\n",
    "tweetPattern = r''' (?x)\t# set flag to allow verbose regexps\n",
    "      (?:https?://|www)\\S+      # simple URLs\n",
    "      | (?::-\\)|;-\\))\t\t# small list of emoticons\n",
    "      | &(?:amp|lt|gt|quot);    # XML or HTML entity\n",
    "      | \\#\\w+                 # hashtags\n",
    "      | @\\w+                  # mentions   \n",
    "      | \\d+:\\d+               # timelike pattern\n",
    "      | \\d+\\.\\d+              # number with a decimal\n",
    "      | (?:\\d+,)+?\\d{3}(?=(?:[^,]|$))   # number with a comma\n",
    "      | (?:[A-Z]\\.)+                    # simple abbreviations\n",
    "      | (?:--+)               # multiple dashes\n",
    "      | \\w+(?:-\\w+)*          # words with internal hyphens or apostrophes\n",
    "      | ['\\\".?!,:;/]+         # special characters\n",
    "      '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example tweets\n",
    "tweet1 = \"@natalieohayre I agree #hc09 needs reform- but not by crooked politicians who r clueless about healthcare! #tcot #fishy NO GOV'T TAKEOVER!\"\n",
    "tweet2 = \"To Sen. Roland Burris: Affordable, quality health insurance can't wait http://bit.ly/j63je #hc09 #IL #60660\"\n",
    "tweet3 = \"RT @karoli: RT @Seriou: .@whitehouse I will stand w/ Obama on #healthcare,  I trust him. #p2 #tlot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@natalieohayre', 'I', 'agree', '#hc09', 'needs', 'reform', 'but', 'not', 'by', 'crooked', 'politicians', 'who', 'r', 'clueless', 'about', 'healthcare', '!', '#tcot', '#fishy', 'NO', 'GOV', \"'\", 'T', 'TAKEOVER', '!']\n",
      "['To', 'Sen', '.', 'Roland', 'Burris', ':', 'Affordable', ',', 'quality', 'health', 'insurance', 'can', \"'\", 't', 'wait', 'http://bit.ly/j63je', '#hc09', '#IL', '#60660']\n",
      "['RT', '@karoli', ':', 'RT', '@Seriou', ':', '.', '@whitehouse', 'I', 'will', 'stand', 'w', '/', 'Obama', 'on', '#healthcare', ',', 'I', 'trust', 'him', '.', '#p2', '#tlot']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.regexp_tokenize(tweet1,tweetPattern))\n",
    "print(nltk.regexp_tokenize(tweet2,tweetPattern))\n",
    "print(nltk.regexp_tokenize(tweet3,tweetPattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@natalieohayre', 'I', 'agree', '#hc09', 'needs', 'reform', '-', 'but', 'not', 'by', 'crooked', 'politicians', 'who', 'r', 'clueless', 'about', 'healthcare', '!', '#tcot', '#fishy', 'NO', \"GOV'T\", 'TAKEOVER', '!']\n"
     ]
    }
   ],
   "source": [
    "# NLTK built-in tokenizer (more detailed version from TweetMotif)\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "ttokenizer = TweetTokenizer()\n",
    "print(ttokenizer.tokenize(tweet1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr', '.', 'Black', 'and', 'Mrs', '.', 'Brown', 'attended', 'the', 'lecture', 'by', 'Dr', '.', 'Gray', ',', 'but', 'Gov', '.', 'White', 'wasn', 't', 'there', '.']\n"
     ]
    }
   ],
   "source": [
    "sent = \"Mr. Black and Mrs. Brown attended the lecture by Dr. Gray, but Gov. White wasn't there.\"\n",
    "print(nltk.regexp_tokenize(sent, pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
